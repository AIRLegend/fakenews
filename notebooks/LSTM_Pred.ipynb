{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red neuronal para el análisis del contenido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando los datos procesados en [Cleaning_news_index](Cleaning_news_index.ipynb), vamos a construir una NN con Keras para clasificar los textos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import style\n",
    "#Nicer style\n",
    "style.use('seaborn') \n",
    "\n",
    "from tensorflow import keras as k\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data es DataFrame con los contenidos y títulos representados como listas de enteros. Estos enteros se corresponden con los índices de las palabras dentro del vocabulario de Word2Vec. Durante el pre-procesado se han *padeado* las secuencias a una longitud fija para poder pasárselo a la red neuronal.\n",
    "\n",
    "Por cuestiones de memoria, la conversión del índice a embeddings se hará en la primera capa de la red en vez de pasar los vectores en el DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../data/news_proc.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>one_hot_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fake</td>\n",
       "      <td>[2458, 4, 27, 17625, 12, 328, 5, 25587, 416, 1...</td>\n",
       "      <td>[1732, 258, 27, 7196, 2154, 4192, 88, 43, 13, ...</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fake</td>\n",
       "      <td>[3836, 22, 506, 3059, 67, 2, 2941, 2429, 33, 3...</td>\n",
       "      <td>[6117, 13034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fake</td>\n",
       "      <td>[7, 37481, 39852, 483, 423, 24, 42, 1837, 81, ...</td>\n",
       "      <td>[6117, 13034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fake</td>\n",
       "      <td>[7, 1790, 659, 24, 846, 6786, 5428, 17, 52, 44...</td>\n",
       "      <td>[13341, 1421, 1992, 1178, 8704, 11, 13034, 387...</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fake</td>\n",
       "      <td>[59, 24, 216, 529, 104, 11, 34925, 133, 31, 24...</td>\n",
       "      <td>[13034, 43, 39049, 43, 76, 1588, 38, 23, 8158,...</td>\n",
       "      <td>[0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                            content  \\\n",
       "0  fake  [2458, 4, 27, 17625, 12, 328, 5, 25587, 416, 1...   \n",
       "1  fake  [3836, 22, 506, 3059, 67, 2, 2941, 2429, 33, 3...   \n",
       "2  fake  [7, 37481, 39852, 483, 423, 24, 42, 1837, 81, ...   \n",
       "3  fake  [7, 1790, 659, 24, 846, 6786, 5428, 17, 52, 44...   \n",
       "4  fake  [59, 24, 216, 529, 104, 11, 34925, 133, 31, 24...   \n",
       "\n",
       "                                               title one_hot_label  \n",
       "0  [1732, 258, 27, 7196, 2154, 4192, 88, 43, 13, ...     [0, 1, 0]  \n",
       "1  [6117, 13034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     [0, 1, 0]  \n",
       "2  [6117, 13034, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]     [0, 1, 0]  \n",
       "3  [13341, 1421, 1992, 1178, 8704, 11, 13034, 387...     [0, 1, 0]  \n",
       "4  [13034, 43, 39049, 43, 76, 1588, 38, 23, 8158,...     [0, 1, 0]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Esta parte se corresponde al testeo previo a encontrar una arquitectura adecuada.\n",
    "</div>\n",
    "\n",
    "Eliminar sobrerrepresentación de los fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfake = data[data['type'] == 'fake']\n",
    "dtrue = data[data['type'] == 'truth']\n",
    "dclic = data[data['type'] == 'click']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([dfake.head(10000), dtrue, dclic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dfake, dtrue, dclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_content = len(data['content'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1866"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_size_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Title len\n",
    "len(data['title'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(\"../data/GoogleNews-vectors-negative300.bin.gz\", binary=True,\n",
    "                                          limit=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_mat = np.zeros((50000,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in model.vocab.items():\n",
    "    embed_mat[e.index] = model[i]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data[['content', 'title']], data['one_hot_label'], test_size=0.4, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title = X_train['title']\n",
    "X_train_content = X_train['content']\n",
    "\n",
    "X_test_title = X_test['title']\n",
    "X_test_content = X_test['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo\n",
    "\n",
    "* inputs: `[TITULO, CONTENT]`\n",
    "* outputs: `label` (one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arquitectura de la red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'functional'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode is not 'functional':\n",
    "    model = k.models.Sequential()\n",
    "\n",
    "    model.add(k.layers.Embedding(50000, 300, input_length=1866,\n",
    "                        weights=[embed_mat], trainable=False))\n",
    "\n",
    "    #model.add(k.layers.LSTM(300, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "    model.add(k.layers.Dense(300, activation='relu'))\n",
    "    model.add(k.layers.Dense(200, activation='relu'))\n",
    "    model.add(k.layers.Dense(100, activation='relu'))\n",
    "\n",
    "    model.add(k.layers.Flatten())\n",
    "    model.add(k.layers.Dense(3, activation='softmax'))\n",
    "    \n",
    "else:\n",
    "    \n",
    "    #input_title\n",
    "    title_input = k.layers.Input(shape=(14,), name='title_input')\n",
    "    inp = k.layers.Embedding(output_dim=300, input_dim=50000, \n",
    "                             weights=[embed_mat], trainable=False)(title_input)\n",
    "    x = k.layers.LSTM(100)(inp)\n",
    "    \n",
    "    \n",
    "    #input_content\n",
    "    content_input = k.layers.Input(shape=(1866,), name='content_input')\n",
    "    inp2 = k.layers.Embedding(output_dim=300, input_dim=50000, \n",
    "                             weights=[embed_mat], trainable=False)(content_input)\n",
    "    x2 = k.layers.LSTM(100, return_sequences=True)(inp2)\n",
    "    x2 = k.layers.LSTM(100)(x2)\n",
    "    \n",
    "    \n",
    "    #Merge\n",
    "    x = k.layers.concatenate([x, x2])\n",
    "    \n",
    "    #Common part\n",
    "    x = k.layers.Dense(64, activation='relu')(x)\n",
    "    \n",
    "    out = k.layers.Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model = k.models.Model(inputs=[title_input, content_input], outputs=[out])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(k.optimizers.Adam(lr=0.1), loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "content_input (InputLayer)      (None, 1866)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "title_input (InputLayer)        (None, 14)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_17 (Embedding)        (None, 1866, 300)    15000000    content_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "embedding_16 (Embedding)        (None, 14, 300)      15000000    title_input[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                  (None, 1866, 100)    160400      embedding_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_14 (LSTM)                  (None, 100)          160400      embedding_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lstm_16 (LSTM)                  (None, 100)          80400       lstm_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 200)          0           lstm_14[0][0]                    \n",
      "                                                                 lstm_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 64)           12864       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 3)            195         dense_13[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 30,414,259\n",
      "Trainable params: 414,259\n",
      "Non-trainable params: 30,000,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fit = [np.asarray(X_train_title.tolist()), \n",
    "             np.asarray(X_train_content.tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8721 samples, validate on 3738 samples\n",
      "Epoch 1/15\n",
      "8721/8721 [==============================] - 606s 70ms/step - loss: 7.7411 - acc: 0.5137 - val_loss: 8.1841 - val_acc: 0.4922\n",
      "Epoch 2/15\n",
      "8721/8721 [==============================] - 605s 69ms/step - loss: 7.8216 - acc: 0.5147 - val_loss: 8.1841 - val_acc: 0.4922\n",
      "Epoch 3/15\n",
      " 256/8721 [..............................] - ETA: 8:35 - loss: 7.7442 - acc: 0.5195"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-61540a2c2e27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m hist = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=64, epochs=15,\n\u001b[0;32m----> 2\u001b[0;31m           callbacks = [k.callbacks.EarlyStopping(monitor='val_acc', patience=2)], validation_split=0.3)\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1637\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1638\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1639\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1641\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m           \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m           \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 2986\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   2987\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hist = model.fit(x=train_fit, y=np.asarray(y_train.tolist()), batch_size=64, epochs=15,\n",
    "          callbacks = [k.callbacks.EarlyStopping(monitor='val_acc', patience=2)], validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(hist.history['val_acc'], label='val_acc')\n",
    "plt.plot(hist.history['acc'], label='train_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(np.asarray(X_test.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_score(np.array(y_test.tolist()), test_pred.round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the model\n",
    "\n",
    "scores = model.evaluate(np.array(X_test.tolist()), np.array(y_test.tolist()),\n",
    "                        batch_size=32)\n",
    "print('Loss:', scores[0])\n",
    "print('Accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
